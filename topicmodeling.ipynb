{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a wordflow showing how to do LDA topic model from reading files, data pre-processing (text cleaning and getting ready for NLP), building LDA model and visualizing LDA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import useful packages\n",
    "import operator\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "import nltk\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Files into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read files into spark, producing a DataFrame has two columns  FILENAME | CONTENT\n",
    "df = spark.read.csv(\n",
    "    \"input_file.csv\", header=True, mode=\"DROPMALFORMED\", schema=schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing LDA model, we should preprocess the text to get ready for analyzing. First, we should count number of words in each text and then remove the corpus with too few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a column of word count to remove the rows with too few number of words\n",
    "df_wc = df.withColumn('wordCount', fn.size(fn.split(fn.col('CONTENT'), \" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better view distribution of word count, let's register the dataframe as temp view to be used by SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# register the Spark DataFrame as a Temp view to be used by SQL\n",
    "df_wc.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- to see the distribution of word count so that documents with too few words can be removed\n",
    "SELECT l.wordCount\n",
    "       ,sum(s.count) as subtotal\n",
    "       ,sum(s.count)*100.0/(select sum(count) from (select round(wordCount*1.0/5)*5 as wordCount,count(filename) as count from df group by round(wordCount*1.0/5)*5)) as percent      \n",
    "FROM\n",
    "(\n",
    "select round(wordCount*1.0/5)*5 as wordCount\n",
    "       ,count(filename) as count\n",
    "from df\n",
    "group by round(wordCount*1.0/5)*5\n",
    ")l\n",
    "LEFT JOIN\n",
    "(\n",
    "select round(wordCount*1.0/5)*5 as wordCount\n",
    "       ,count(filename) as count\n",
    "from df\n",
    "group by round(wordCount*1.0/5)*5\n",
    ")s on l.wordCount >= s.wordCount\n",
    "group by l.wordCount\n",
    "order by l.wordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my sample documents, 5% of documents have word count less than 25. Let's remove documents which have wordcount less than 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove 5% of the documents, which is wordCount less than 25\n",
    "df_wc_gt25 = df_wc.where(df_wc.wordCount > 25) \n",
    "print \"before filtering wordCount, there are %s txt files\" %df_wc.count()\n",
    "print \"after filtering out wordCount <= 30, there are %s txt files\" %df_wc_gt25.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing unnecesary documents, text should be tokenized to be analyzed. Now, let's tokenize from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer().setInputCol('CONTENT').setOutputCol('words')\n",
    "tokenized = tokenizer.transform(df_wc_gt25)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenizing the text, we shoud remove stopwords like \"a, an, the, we're\" etc. Let's download stopwords list from NLTK. We can also add customized stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct stopwords list\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add cutomized stopwords into stopwords list\n",
    "\n",
    "stop_words.extend([u\"twenty\",u\"thirty\",u\"forty\",u'sixty',u'seventy',u'eighty',u'eleven', u'twelve',u'thirteen',u'fourteen',u'fifteen',u'sixteen',u'seventeen',u'eighteen',u'nineteen'])\n",
    "\n",
    "stop_words.extend([u'january', u'february', u'march', u'april', u'may', u'june', u'july', u'august', u'september', u'october', u'november', u'december'])\n",
    "\n",
    "stop_words.extend([u'monday', u'tuesday', u'wednesday', u'thursday', u'friday', u'saturday', u'sunday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "sw_filteredDF = sw_filter.transform(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing stopwords, let's lemmatize the filtered token. \n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word. https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build class to lemmatize the filtered token\n",
    "class lemmatizer():\n",
    "# define own function of lemmatize\n",
    "  def lemmatize(self, row, Input, Output):\n",
    "    # to add a column of lemmatized words, let's turn RDD's row into dictionary since tuple is not mutable\n",
    "    rowDict = row.asDict()\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordvec = []\n",
    "    # iterate input columns words and get a list of lemmatized word\n",
    "    for word in rowDict[Input]:\n",
    "      wordvec.append(lemmatizer.lemmatize(word))\n",
    "    # add new list of lemmatized word into row dictionary\n",
    "    rowDict[Output] = wordvec\n",
    "    # turn row dictionary back to Row\n",
    "    newrow = Row(**rowDict)\n",
    "    return newrow\n",
    "\n",
    "  def transform(self, data, Input, Output):\n",
    "    lemmatizedDF = data.rdd.map(lambda x: self.lemmatize(x, Input, Output)).toDF()\n",
    "    return lemmatizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lemmatize the filtered token\n",
    "lemmatizedDF = lemmatizer().transform(sw_filteredDF,\"filtered\", \"lemmatized\")\n",
    "lemmatizedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After above steps, let's build TF-IDF. For more information about TF-IDF, please look at https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF\n",
    "# we will remove words that appear in 5 docs or less\n",
    "cv = CountVectorizer(vocabSize=2**17)\\\n",
    "  .setInputCol(\"lemmatized\")\\\n",
    "  .setOutputCol(\"tf\")\\\n",
    "  .setMinDF(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IDF\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit TF and IDF into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF & IDF after lemmatizer\n",
    "pipepline = Pipeline(stages=[cv, idf]).fit(lemmatizedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_tfidf = pipepline.transform(lemmatizedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about LDA model, please refer to https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "Now, let's build LDA model -- keep tuning the number_of_topics and maxIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_topics = 7\n",
    "lda = LDA(k=num_of_topics, seed=12344, optimizer=\"em\", featuresCol=\"tfidf\", maxIter = 100).setTopicDistributionCol(\"topicDistributionCol\")\n",
    "ldamodel = lda.fit(result_tfidf)\n",
    "transformed = ldamodel.transform(result_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building LDA model, next step is to visualize the result of LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each word is represented by a number, we need to convert the number back to word. So, let's get vocabulary of CountVectorizer and transform to dictionary for easily use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = pipepline.stages[0].vocabulary\n",
    "vocab_dict = {k: v for k, v in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topics X words\n",
    "class topic_describe():\n",
    "  wordNumber = 5\n",
    "  def setWordNumber(self, num):\n",
    "    self.wordNumber = num\n",
    "    \n",
    "  def topic_render(self, row):\n",
    "    result = [[row.topic]]\n",
    "    dictionary = dict(zip(row.termIndices, row.termWeights))\n",
    "    newDict = dict(sorted(dictionary.iteritems(), key=operator.itemgetter(1), reverse=True)[:self.wordNumber])\n",
    "    termwords = []\n",
    "    termweights = []\n",
    "    for key, value in newDict.iteritems():\n",
    "        termwords.append(vocab_dict[key])\n",
    "        termweights.append(value)\n",
    "    result.extend([termwords, termweights])\n",
    "    return result\n",
    "  \n",
    "  def topic_by_word(self, ldamodel):\n",
    "    topicIndices = ldamodel.describeTopics()\n",
    "    topics_final = topicIndices.rdd.map(self.topic_render).toDF([\"topic\",\"termwords\",\"termweights\"])\n",
    "    return topics_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display topics X words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topicresult = topic_describe()\n",
    "topicresult.setWordNumber(7) # setting how many words to describe topics\n",
    "display(topicresult.topic_by_word(ldamodel).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get Documents X Topics by FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class docuByTopicRender():\n",
    "  TopicNumber = 0\n",
    "  InputCol = ''\n",
    "  OutputCol = ''\n",
    "  cutoff = 0\n",
    "  def setTopicNumber(self, num):\n",
    "    self.TopicNumber = num\n",
    "    \n",
    "  def setInputCol(self, name):\n",
    "    self.InputCol = name\n",
    "  \n",
    "  def setOutputCol(self, name):\n",
    "    self.OutputCol = name\n",
    "    \n",
    "  def setCutoff(self, number):\n",
    "    self.cutoff = number\n",
    "  \n",
    "  #convert vector of TopicDistributionCol to a readable format - columns of topics with list of probability as value\n",
    "  def topicDistConvertCols(self, data):\n",
    "    for i in range(self.TopicNumber):\n",
    "      ithelement=udf(lambda v:float(v[i]),FloatType())\n",
    "      data = data.withColumn('Topic %s' %i, ithelement(self.InputCol))\n",
    "    return data\n",
    "\n",
    "  def topicColsCombined(self, row):\n",
    "    topiclist = ['Topic %s' %i for i in range(self.TopicNumber)]\n",
    "    problist = []\n",
    "    for topic in topiclist:\n",
    "      problist.append(row[topic])\n",
    "    topicDict = dict(zip(topiclist, problist)) # combine topics with probabilities as one column and filtered based on cutoff\n",
    "    topicDoc = {k: v for k, v in topicDict.iteritems() if v >= self.cutoff}\n",
    "    return [row['FILENAME'], topicDoc]\n",
    "\n",
    "  def docuByTopic(self, data):\n",
    "    transformedWithTopics = self.topicDistConvertCols(data)\n",
    "    return transformedWithTopics.rdd.map(lambda x: self.topicColsCombined(x)).toDF(['FILENAME', self.OutputCol])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display Documents X Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docAndTopic = docuByTopicRender()\n",
    "docAndTopic.setTopicNumber(num_of_topics)\n",
    "docAndTopic.setInputCol('topicDistributionCol')\n",
    "docAndTopic.setOutputCol('topicDoc')\n",
    "docAndTopic.setCutoff(0.1) # only showing the topics which is greater than a cutoff (0.1?)\n",
    "dff = docAndTopic.docuByTopic(transformed)\n",
    "display(dff.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't finished. We should keep tuning the LDA model based on the results and finally optimize the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "amex_topcmodeling",
  "notebookId": 3896267403004469
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
